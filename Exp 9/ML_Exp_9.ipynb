{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m3jWMsoZb_9K"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_digits\n",
        "import pandas as pd\n",
        "#loading digit dataset\n",
        "dataset = load_digits()\n",
        "dataset.keys()\n",
        "dataset.data.shape # 64 columns\n",
        "dataset.data[0] # flat 1D array of 64 pixels\n",
        "dataset.data[0].reshape(8,8) # convert 2D arrays of pixels\n",
        "from matplotlib import pyplot as plt\n",
        "%matplotlib inline\n",
        "plt.gray()\n",
        "plt.matshow(dataset.data[0].reshape(8,8)) # matrix show -- can change 0 to 1,2,3,4...etc\n",
        "plt.matshow(dataset.data[5].reshape(8,8))\n",
        "import numpy as np\n",
        "np.unique(dataset.target) # 10 classses 0 to 9\n",
        "dataset.target[:5] # class\n",
        "df = pd.DataFrame(dataset.data, columns=dataset.feature_names) df.head()\n",
        "dataset.target\n",
        "df.describe() # 0 min value 16 max value\n",
        "X = df\n",
        "y = dataset.target\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler() # values scaled like -1 to 1\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "X_scaled\n",
        "print(\"Models accuracy by considering all features\")\n",
        "print(\"\\n\\n*** Done By Didar Abbas-211P036***\\n\\n\") from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2,\n",
        "random_state=30)\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "model.score(X_test, y_test)\n",
        "#Applying PCA for columns reduction\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "pca = PCA(0.95) # retain 95% features\n",
        "X_pca = pca.fit_transform(X)\n",
        "X_pca.shape # now see 29 column got whereas total 64 column\n",
        "X_pca\n",
        "pca.explained_variance_ratio_\n",
        "pca.n_components_ # tell column selected or computed\n",
        "print(\"Models accuracy by considering 29 important features\")\n",
        "print(\"\\n\\n*** Done By Didar Abbas-211P036***\\n\\n\")\n",
        "X_train_pca, X_test_pca, y_train, y_test = train_test_split(X_pca, y, test_size=0.2,\n",
        "random_state=30)\n",
        "from sklearn.linear_model import LogisticRegression model = LogisticRegression(max_iter=2000)\n",
        "model.fit(X_train_pca, y_train)\n",
        "model.score(X_test_pca, y_test)\n",
        "#selecting oly two columns\n",
        "pca = PCA(n_components=2)\n",
        "X_pca = pca.fit_transform(X)\n",
        "X_pca.shape\n",
        "X_pca\n",
        "pca.explained_variance_ratio_\n",
        "print(\"Models accuracy by considering only 2 features features\")\n",
        "print(\"\\n\\n*** Done By Didar Abbas-211P036***\\n\\n\")\n",
        "X_train_pca, X_test_pca, y_train, y_test = train_test_split(X_pca, y, test_size=0.2, random_state=30)\n",
        "model = LogisticRegression(max_iter=2000)\n",
        "model.fit(X_train_pca, y_train)\n",
        "model.score(X_test_pca, y_test)\n",
        "#here the accuracy is reduced because of too less features\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# PostLab:\n",
        "from sklearn.datasets import load_iris\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "print(\"\\n\\n*** Done By Didar Abbas-211P036***\\n\\n\")\n",
        "# Load Iris dataset\n",
        "dataset = load_iris()\n",
        "dataset.keys()\n",
        "\n",
        "# Explore data structure\n",
        "dataset.data.shape # 150 samples, 4 features\n",
        "dataset.data[0] # First sample (4 feature values)\n",
        "dataset.target[:5] # First 5 target values (species)\n",
        "\n",
        "# Convert to DataFrame\n",
        "df = pd.DataFrame(dataset.data, columns=dataset.feature_names)\n",
        "df.head()\n",
        "\n",
        "# Dataset description\n",
        "df.describe()\n",
        "\n",
        "# Features and Target\n",
        "X = df\n",
        "y = dataset.target\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Train/Test Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2,\n",
        "random_state=30)\n",
        "\n",
        "# Model Training with All Features\n",
        "print(\"Model accuracy using all features:\")\n",
        "model = LogisticRegression(max_iter=2000)\n",
        "model.fit(X_train, y_train)\n",
        "print(f\"Accuracy: {model.score(X_test, y_test):.4f}\\n\\n\")\n",
        "\n",
        "# PCA to retain 95% variance\n",
        "pca = PCA(0.95)\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "print(f\"PCA Shape: {X_pca.shape}\") # Reduced number of features\n",
        "print(f\"Explained Variance Ratio: {pca.explained_variance_ratio_}\")\n",
        "print(f\"Number of components: {pca.n_components_}\")\n",
        "\n",
        "# Train/Test Split for PCA-reduced data\n",
        "X_train_pca, X_test_pca, y_train, y_test = train_test_split(X_pca, y, test_size=0.2,\n",
        "random_state=30)\n",
        "# Model Training with PCA-reduced features\n",
        "print(\"Model accuracy using 95% variance retained features:\")\n",
        "model.fit(X_train_pca, y_train)\n",
        "print(f\"Accuracy: {model.score(X_test_pca, y_test):.4f}\\n\\n\")\n",
        "\n",
        "# PCA with only 1 components\n",
        "pca = PCA(n_components=1)\n",
        "X_pca_1 = pca.fit_transform(X_scaled)\n",
        "print(f\"PCA Shape (1 components): {X_pca_1.shape}\")\n",
        "print(f\"Explained Variance Ratio: {pca.explained_variance_ratio_}\")\n",
        "\n",
        "# Train/Test Split for 1-component PCA data\n",
        "X_train_pca_1, X_test_pca_1, y_train, y_test = train_test_split(X_pca_1, y, test_size=0.2,\n",
        "random_state=30)\n",
        "# Model Training with 1-component PCA features\n",
        "print(\"Model accuracy using only 1 components:\")\n",
        "model.fit(X_train_pca_1, y_train)\n",
        "print(f\"Accuracy: {model.score(X_test_pca_1, y_test):.4f}\")\n",
        "#There is not change in acccuracy beacause dataset itself has less columns\n"
      ],
      "metadata": {
        "id": "BVJtnO--caXi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}